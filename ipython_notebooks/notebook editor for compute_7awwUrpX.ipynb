{
  "metadata": {
    "kernelspec": {
      "name": "py-dku-containerized-venv-mlflow-cpu-m-1-cpu-4gb-ram",
      "display_name": "Python in CPU-M-1-cpu-4Gb-Ram (env mlflow)",
      "language": "python"
    },
    "associatedRecipe": "compute_7awwUrpX",
    "creator": "chris.helmus@dataiku.com",
    "createdOn": 1673389196553,
    "tags": [
      "recipe-editor"
    ],
    "customFields": {},
    "hide_input": false,
    "modifiedBy": "chris.helmus@dataiku.com",
    "language_info": {
      "name": "python",
      "version": "3.8.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1,
  "cells": [
    {
      "execution_count": 2,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# -*- coding: utf-8 -*-\nimport dataiku\nimport pandas as pd, numpy as np\nfrom dataiku import pandasutils as pdu\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import roc_auc_score\n# Read recipe inputs\nmy_dataset \u003d dataiku.Dataset(\"flight_data_prepared\")\ndf \u003d my_dataset.get_dataframe()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "text": "/opt/dataiku/python/dataiku/core/schema_handling.py:17: FutureWarning: In the future `np.bool` will be defined as the corresponding NumPy scalar.  (This may have returned Python scalars in past versions.\n  \u0027boolean\u0027: np.bool\n",
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "module \u0027numpy\u0027 has no attribute \u0027bool\u0027",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m\u003cipython-input-2-7ea4184afa98\u003e\u001b[0m in \u001b[0;36m\u003ccell line: 2\u003e\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# -*- coding: utf-8 -*-\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----\u003e 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mdataiku\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdataiku\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandasutils\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpdu\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipeline\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/dataiku/python/dataiku/__init__.py\u001b[0m in \u001b[0;36m\u003cmodule\u003e\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mremoterun\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mis_container_exec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---\u003e 13\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_dataset_writer_atexit_handler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschema_handling\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_schema_from_df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/dataiku/python/dataiku/core/dataset.py\u001b[0m in \u001b[0;36m\u003cmodule\u003e\u001b[0;34m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;31m# Module code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---\u003e 34\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdataiku\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mflow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbase\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema_handling\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdkuio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdataiku\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplatform_exec\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mread_dku_json\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdataiku\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdkujson\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdump_to_filepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mload_from_filepath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/dataiku/python/dataiku/core/schema_handling.py\u001b[0m in \u001b[0;36m\u003cmodule\u003e\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;34m\u0027float\u0027\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;34m\u0027double\u0027\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---\u003e 17\u001b[0;31m     \u001b[0;34m\u0027boolean\u0027\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m }\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/dataiku/code-env/lib/python3.8/site-packages/numpy/__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(attr)\u001b[0m\n\u001b[1;32m    282\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mTester\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 284\u001b[0;31m         raise AttributeError(\"module {!r} has no attribute \"\n\u001b[0m\u001b[1;32m    285\u001b[0m                              \"{!r}\".format(__name__, attr))\n\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module \u0027numpy\u0027 has no attribute \u0027bool\u0027"
          ]
        }
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#-----------------------------------------------------------------\n# Dataset Settings\n#-----------------------------------------------------------------\n\n# Select a subset of features to use for training\nSCHEMA \u003d {\n    \u0027target\u0027: \u0027Late_avg\u0027,\n    \u0027features_num\u0027: [\u0027dep_month\u0027, \u0027dep_woy\u0027, \u0027dep_hour\u0027,\u0027Distance\u0027],\n    \u0027features_cat\u0027: [\u0027UniqueCarrier\u0027, \u0027Origin\u0027,\u0027Dest\u0027,\u0027Late\u0027]\n}"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#-----------------------------------------------------------------\n# Preprocessing on Training Set\n#-----------------------------------------------------------------\n\n# Numerical variables\ndf_num \u003d df[SCHEMA[\u0027features_num\u0027]]\n\ntrf_num \u003d Pipeline([\n    (\u0027imp\u0027, SimpleImputer(strategy\u003d\u0027mean\u0027)),\n    (\u0027sts\u0027, StandardScaler()),\n])\n\n# Categorical variables\ndf_cat \u003d df[SCHEMA[\u0027features_cat\u0027]]\n\ntrf_cat \u003d OneHotEncoder(handle_unknown\u003d\"ignore\")\n\npreprocessor \u003d ColumnTransformer(\n    transformers\u003d[\n        (\"num\", trf_num, SCHEMA[\u0027features_num\u0027]),\n        (\"cat\", trf_cat, SCHEMA[\u0027features_cat\u0027])\n    ]\n)"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import dataiku\nimport mlflow\n\n\nproject \u003d dataiku.api_client().get_default_project()\nmanaged_folder \u003d project.get_managed_folder(\u0027R47GTRDp\u0027)\nexperiment_name \u003d \"my_experiment\"\n\nwith project.setup_mlflow(managed_folder\u003dmanaged_folder) as mlflow:\n    mlflow.set_experiment(experiment_name)\n\n    # activate Mflow autologging\n    mlflow.sklearn.autolog()\n\n    with mlflow.start_run(run_name\u003d\"my_run\"):\n        clf \u003d Pipeline(steps\u003d[(\"preprocessor\", preprocessor), (\"clf\", RandomForestRegressor())])\n\n        param_grid \u003d {\n            \"clf__max_depth\"        : [3],\n            \"clf__max_features\"     : [1],\n            \"clf__min_samples_split\": [2],\n            \"clf__min_samples_leaf\" : [1],\n            \"clf__bootstrap\"        : [False],\n            \"clf__n_estimators\"     : [10]\n        }\n\n        gs \u003d GridSearchCV(clf, param_grid\u003dparam_grid, n_jobs\u003d-1, scoring\u003d\u0027neg_mean_absolute_error\u0027, cv\u003d3)\n        X \u003d df[SCHEMA[\u0027features_num\u0027] + SCHEMA[\u0027features_cat\u0027]]\n        #Y \u003d df[SCHEMA[\u0027target\u0027]].values\n        Y \u003d df[SCHEMA[\u0027target\u0027]]\n        gs.fit(X, Y)\n        clf \u003d gs.best_estimator_"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "client \u003d dataiku.api_client()\nproject \u003d client.get_project(\u0027MLFLOWCH\u0027)\n# Get or create saved models\nif dataiku.get_custom_variables()[\"saved_model_id\"] \u003d\u003d \"\":\n    saved_model \u003d project.create_mlflow_pyfunc_model(\"mlflow_model\", \"REGRESSION\")\n    project.update_variables({\"saved_model_id\": saved_model.id})\nelse:\n    saved_model \u003d project.get_saved_model(dataiku.get_custom_variables()[\"saved_model_id\"])"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "experiment_folder \u003d dataiku.Folder(\"R47GTRDp\")\n#model_dir \u003d experiment_folder.get_path()+\"/my_experiment_tracking/my_experiment/my_run_mEN/artifacts/model/\"\nmodel_dir \u003d\"my_experiment/my_run_mEN/artifacts/model/\"\nmlflow_version \u003d saved_model.import_mlflow_version_from_managed_folder(dataiku.get_custom_variables()[\"custom_model_version\"], \"R47GTRDp\", model_dir,code_env_name\u003d\"python36\")\nproject.update_variables({\"custom_model_version\": int(dataiku.get_custom_variables()[\"custom_model_version\"]) + 1})"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "mlflow_version.set_core_metadata(SCHEMA[\u0027target\u0027],get_features_from_dataset\u003d\"flight_ground_truth_prepared\")\nmlflow_version.evaluate(\"flight_ground_truth_prepared\")"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import dataiku\ntest \u003d dataiku.Dataset(\"flight_ground_truth_prepared\")\ndf_2 \u003d test.get_dataframe()\ndf_2.describe()"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\u0027\u0027\u0027\n#extra testing to confirm if i can properly load file\nimport dataiku\nhandle \u003d dataiku.Folder(\"3hOB5aod\")\n# pass a partition identifier if the folder is partitioned\npaths \u003d handle.list_paths_in_partition()\npaths\n\nwith handle.get_download_stream(\"experiment_2/my_run_jHL/artifacts/model/conda.yaml\") as f:\n    data \u003d f.read()\n\noutput_handle \u003d dataiku.Folder(\"WISPen39\")\n\nwith output_handle.get_writer(\"output.yaml\") as w:\n    w.write(data)\n    \u0027\u0027\u0027"
      ],
      "outputs": []
    }
  ]
}